{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from model_config import *\n",
    "from model_packages import *\n",
    "from model_utils import *\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "# complete dataset\n",
    "model_input_data= pd.read_pickle(data_folder+'static_and_dynamic_features_5000.pkl')\n",
    "\n",
    "# poi features\n",
    "pois= pd.read_pickle(data_folder+'new_pois_data_all_sites.pkl')\n",
    "\n",
    "# census features\n",
    "census = pd.read_pickle('./data/non_baseline_features.pkl')\n",
    "\n",
    "# data with UK regions \n",
    "regions= gpd.read_file('./data/NUTS_RG_20M_2021_4326.geojson')\n",
    "\n",
    "# constants for columns\n",
    "trgt_ftrs=['people_counter_data']\n",
    "dynmc_ftrs=['total_trip_count']\n",
    "wthr_ftrs=['tavg']\n",
    "othr_ftrs=['Dog']    \n",
    "catg_ftrs=['Date', 'site']\n",
    "natr_ftrs_non_corr=['accessible_green_space_area','PROW_Total_length_km','waterside_length_km']\n",
    "season=[ 'autumn', 'spring', 'summer']\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_season(month):\n",
    "    if 3 <= month <= 5:\n",
    "        return 'spring'\n",
    "    elif 6 <= month <= 8:\n",
    "        return 'summer'\n",
    "    elif 9 <= month <= 11:\n",
    "        return 'autumn'\n",
    "    else:\n",
    "        return 'winter'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_season_columns(df):\n",
    "    df['Month'] = df['Date'].str.split('-', expand=True)[1].astype(int)\n",
    "    df['Season'] = df['Month'].apply(get_season)\n",
    "    season_columns = pd.get_dummies(df['Season'])\n",
    "    df = pd.concat([df, season_columns], axis=1)\n",
    "    df.drop(columns=['Month', 'Season', 'winter'], inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_input_data(input_data):\n",
    "    # raw data wrangling\n",
    "    # create geo dataframe\n",
    "    df= gpd.GeoDataFrame(input_data)\n",
    "\n",
    "    # drop sites that create negative prediction values- to be investigated\n",
    "    df= df[~df['counter'].isin(['Vessey_Pastures', 'Trosley_CP'])]\n",
    "    # assign geometry as centre of buffer\n",
    "    df['geometry']= df.geometry.centroid\n",
    "    # extract lat lon\n",
    "    df['lon']= df.geometry.x\n",
    "    df['lat']= df.geometry.y\n",
    "\n",
    "    # create season columns\n",
    "    df = create_season_columns(df)\n",
    "\n",
    "    # # replace nan values with 0s as Nan not allowed in model training.\n",
    "    df[pois.columns]= df[pois.columns].fillna(0) \n",
    "\n",
    "    # combine minority classes in land_type features \n",
    "    lnd_dict={'major_urban_settings':'urban_settings',\\\n",
    "    'minor_urban_settings':'urban_settings'}    \n",
    "    df['land_type_labels'].replace(lnd_dict,inplace=True)\n",
    "\n",
    "    # create land and habitat feature constants\n",
    "    lnd_ftrs=['land_type_labels_'+x for x in list(df['land_type_labels'].unique())]\n",
    "    hbt_ftrs=['land_habitat_labels_'+x for x in list(df['land_habitat_labels'].unique())]\n",
    "\n",
    "    return df, lnd_ftrs, hbt_ftrs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify sites in each region\n",
    "def find_counter_regions(region_df, counter_df):\n",
    "\n",
    "    # select desired geography level i.e. North East, North West etc\n",
    "    regions=region_df.loc[(region_df['CNTR_CODE']=='UK')&(region_df['LEVL_CODE']==1)]\n",
    "    # create data frame with geometry of region related to each counter location\n",
    "    regions_geom=gpd.sjoin(left_df=counter_df, right_df=regions[['geometry', 'NUTS_NAME']], how='left')\n",
    "\n",
    "    regions_geom= regions_geom[['counter', 'NUTS_NAME']].drop_duplicates()\n",
    "\n",
    "    counter_regions= {}\n",
    "    for region in regions_geom['NUTS_NAME'].unique():\n",
    "        counter_regions[f'{region}']= regions_geom.loc[regions_geom['NUTS_NAME'].isin([region])]\n",
    "    \n",
    "    return counter_regions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering based on coorinates or on land_type_lables\n",
    "\n",
    "def training_test_split(df, cluster_coordinates, k, test_size):\n",
    "    \n",
    "    if cluster_coordinates == True:\n",
    "\n",
    "        # create dummy variables for categorical data\n",
    "        df=pd.get_dummies(df,columns=['land_type_labels'])\n",
    "        df=pd.get_dummies(df,columns=['land_habitat_labels'])\n",
    "\n",
    "        # kmeans clustering on counter coordinates\n",
    "        coords = df[['site','lat','lon']].drop_duplicates()\n",
    "        # coordinates = df[['lat', 'lon']]\n",
    "\n",
    "        # KMeans clustering\n",
    "        kmeans = KMeans(n_clusters=k, init='k-means++', max_iter=300, n_init=10, random_state=0)\n",
    "        coords['cluster_label'] = kmeans.fit_predict(coords[['lat', 'lon']])\n",
    "\n",
    "        # Stratified splitting of the data based on cluster lables\n",
    "        train_set, test_set = train_test_split(coords, test_size=test_size, stratify=coords['cluster_label'])\n",
    "\n",
    "        # subset original data to train and test data based on cluster lables\n",
    "        train_data = df.loc[df.site.isin(train_set.site)]\n",
    "        test_data = df.loc[df.site.isin(test_set.site)]\n",
    "\n",
    "        print(coords['cluster_label'].value_counts())\n",
    "        print(f'Train sites:{list(train_set.site)}, Test sites: {list(test_set.site)}')\n",
    "    \n",
    "    else:\n",
    "        # stratify splitting based on land_type_lable\n",
    "        land_type = df[['site','land_type_labels']].drop_duplicates()\n",
    "        train_set, test_set = train_test_split(land_type, test_size=test_size, stratify=land_type['land_type_labels'])\n",
    "        \n",
    "        # create dummy variables for categorical data\n",
    "        df=pd.get_dummies(df,columns=['land_type_labels'])\n",
    "        df=pd.get_dummies(df,columns=['land_habitat_labels'])\n",
    "\n",
    "        # subset original data to train and test data based on cluster lables\n",
    "        train_data = df.loc[df.site.isin(train_set.site)]\n",
    "        test_data = df.loc[df.site.isin(test_set.site)]\n",
    "\n",
    "        print(f'Train sites:{list(train_set.site)}, Test sites: {list(test_set.site)}')\n",
    "\n",
    "    return df, train_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_sites(geo_df, counter_regions, train_data, test_data):\n",
    "    # Create list of SE and SW counter locations\n",
    "    se_sw_counters = list(counter_regions['South West (England)'].counter) + list(counter_regions['South East (England)'].counter)\n",
    "\n",
    "    # Subset to only sites within SE & SW and in test or train data sets\n",
    "    se_sw_counters_locations_test = geo_df.loc[(geo_df.counter.isin(list(test_data.counter)))]\n",
    "    se_sw_counters_locations_train = geo_df.loc[(geo_df.counter.isin(list(train_data.counter)))]\n",
    "\n",
    "    # Plot the locations\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))  # Adjust the figure size as needed\n",
    "    train_scatter = se_sw_counters_locations_train.plot(ax=ax, markersize=30, color='#fa6401', marker='*', alpha=0.7, label='Train Data', zorder=3)\n",
    "    test_scatter = se_sw_counters_locations_test.plot(ax=ax, markersize=30, color='#902082', marker='o', alpha=0.7, label='Test Data', zorder=3)\n",
    "\n",
    "    # Add Basemap\n",
    "    contextily.add_basemap(ax, crs=geo_df.crs.to_string(), source=contextily.providers.CartoDB.Voyager)\n",
    "\n",
    "    # Customize map appearance\n",
    "    ax.axis('off')\n",
    "    legend = ax.legend(loc='upper right', fontsize='large', title='Site Type', title_fontsize='large', frameon=True)  # Frame added\n",
    "    legend.get_frame().set_color('white')  # Set the frame color to white (or any other color)\n",
    "    legend.get_frame().set_edgecolor('black')  # Set the frame edge color\n",
    "\n",
    "    for handle in legend.legendHandles:  # Set the same alpha for legend markers as in the plot\n",
    "        handle.set_alpha(0.7)\n",
    "\n",
    "    ax.set_title('Train and Test set Counter Locations', fontsize=20)  # Optional title\n",
    "\n",
    "    plt.savefig(f\"./images/train_test_sites.png\", format= 'png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "    plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vif(df, lnd_ftrs, hbt_ftrs):\n",
    "\n",
    "   # create reference category for land habitat and land type\n",
    "   habitat_type_reference= ['mixed_settings', 'Grassland_woodland_bareground']\n",
    "\n",
    "   # select input features\n",
    "   ftrs_to_keep=dynmc_ftrs+wthr_ftrs+othr_ftrs+natr_ftrs_non_corr+\\\n",
    "   lnd_ftrs+hbt_ftrs+catg_ftrs+list(pois.columns)+list(census.columns)\n",
    "\n",
    "   ftrs_to_keep=[x for x in ftrs_to_keep if x not in habitat_type_reference]\n",
    "  \n",
    "   print(list(ftrs_to_keep))\n",
    "   # For each ftr, calculate VIF and save in dataframe\n",
    "   vif_1 = pd.DataFrame()\n",
    "\n",
    "   df_num=df[[x for x in ftrs_to_keep if x not in [target,  'geometry',\n",
    "   'geom_type', 'Date', 'site', 'counter', 'provider']]].select_dtypes(include=np.number).dropna(axis=0)\n",
    "   vif_1[\"VIF Factor\"] = [variance_inflation_factor(df_num.values, i) for \\\n",
    "                     i in range(df_num.shape[1])]\n",
    "   vif_1[\"features\"] = df_num.columns\n",
    "\n",
    "\n",
    "   print(vif_1)\n",
    "\n",
    "   print('+'*100)\n",
    "\n",
    "   ftrs_to_chck=[x for x in ftrs_to_keep if x not in [target,'total_trip_count','tavg',  'geometry',\n",
    "   'geom_type', 'Date', 'site', 'counter', 'provider', 'amenity_holiday_park', 'tourism_yes', 'amenity_waste_basket']]\n",
    "               \n",
    "   df_num_remv_multi_coll=calculate_vif_(df_num[ftrs_to_chck].select_dtypes(include=np.number).dropna(axis=0),\\\n",
    "                                             thresh=10)[0]\n",
    "\n",
    "   vif = pd.DataFrame()\n",
    "   vif[\"VIF Factor\"] = [variance_inflation_factor(df_num_remv_multi_coll.values, i) for \\\n",
    "                     i in range(df_num_remv_multi_coll.shape[1])]\n",
    "   vif[\"features\"] = df_num_remv_multi_coll.columns\n",
    "\n",
    "\n",
    "   #Vif removed features\n",
    "   low_vif_ftrs_df_train_num=list(vif.features.values)\n",
    "\n",
    "   # possibly correlated features\n",
    "   #low_vif_ftrs_df_train_num=ftrs_to_chck\n",
    "               \n",
    "   low_vif_ftrs_df_train_num=low_vif_ftrs_df_train_num+['total_trip_count','tavg']\n",
    "   print(low_vif_ftrs_df_train_num)   \n",
    "\n",
    "   return low_vif_ftrs_df_train_num+[target] \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_dataframe(dataframe, columns_to_drop, drop):\n",
    "    # Drop specified columns\n",
    "    # processed_df= dataframe\n",
    "    if drop == True:\n",
    "        processed_df = dataframe.drop(columns_to_drop, axis=1)\n",
    "\n",
    "        # Rename columns by removing non-alphanumeric characters\n",
    "        processed_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x), inplace=True)\n",
    "    else:\n",
    "        processed_df = dataframe\n",
    "\n",
    "        # Rename columns by removing non-alphanumeric characters\n",
    "        processed_df.rename(columns=lambda x: re.sub('[^A-Za-z0-9_]+', '', x), inplace=True)\n",
    "\n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictions(train_data, test_data, low_vif_ftrs, normalisation):\n",
    "\n",
    "  # normalisation?\n",
    "  if normalisation== True:\n",
    "    norm=True\n",
    "  else:\n",
    "    norm=False  \n",
    "\n",
    "  df_train= train_data[low_vif_ftrs+['counter', 'Date']].dropna(axis=0)\n",
    "  df_test= test_data[low_vif_ftrs+['counter', 'Date']].dropna(axis=0)\n",
    "  # Setup for PyCaret\n",
    "  s = setup(data = df_train.copy(),#[low_vif_ftrs].dropna(axis=0).copy(), \n",
    "          target = 'people_counter_data',\n",
    "          train_size=0.9,\n",
    "          numeric_features = [x for x in list(df_train.select_dtypes(include=np.number).columns)\\\n",
    "                                  if x not in ['people_counter_data']], \n",
    "                                  fold = 3, \n",
    "                                  preprocess= False,\n",
    "                                  normalize=norm,\n",
    "                                  normalize_method = 'robust',\n",
    "                                  remove_outliers=False, \n",
    "                                  remove_multicollinearity=False,\n",
    "                                  multicollinearity_threshold=0.8,\n",
    "                                  feature_selection=False,\n",
    "                                  ignore_features=['Date','site', 'counter'],\n",
    "                                  polynomial_features=False,\n",
    "                                  pca=False, \n",
    "                                  log_experiment=True,\n",
    "                                  experiment_name='reg_experiments', \n",
    "                                  log_plots=True,\n",
    "                                  transformation=False,\n",
    "                                  #   fold_strategy = 'timeseries'\n",
    "                                )\n",
    "  \n",
    "\n",
    "  # Model training and tuning\n",
    "  # best = compare_models(n_select=1, include=['lr'], sort='MAE', fold=5)#, 'rf', 'et', 'gbr', 'lightgbm'], sort='MAE', fold=5)\n",
    "  top5 = compare_models(n_select=5, sort='MAE', fold=5, include=['lr', 'ridge', 'lasso', 'en', 'br', 'kr'])\n",
    "  tuned_top5 = [tune_model(i, n_iter=120, optimize='MAE', fold=5, verbose= False) for i in top5]\n",
    "  blender_specific = blend_models(estimator_list=tuned_top5, fold=5, optimize='MAE')\n",
    "\n",
    "  finalize_blender = finalize_model(blender_specific)\n",
    "  save_model(finalize_blender, data_folder+'voting_regressor_model') \n",
    "   \n",
    "  \n",
    "  # Predictions\n",
    "  print('Performance metrics from training data:')\n",
    "  pred_on_train = predict_model(blender_specific, data=df_train)\n",
    "  pred_on_train.to_pickle(data_folder+'training_predictions.pkl')\n",
    "\n",
    "  print('Performance metrics from test data:')\n",
    "  pred_on_test = predict_model(blender_specific, data=df_test)\n",
    "  pred_on_test.to_pickle(data_folder+'test_predictions.pkl')\n",
    "\n",
    "  # finalise and save model after predictions as finalise trains model on complete data set(not train/test split)\n",
    "  finalize_blender = finalize_model(blender_specific)\n",
    "  save_model(finalize_blender, data_folder+'voting_regressor_model') \n",
    "\n",
    "  \n",
    "  return pred_on_train, pred_on_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_development_pipeline(input_df, test_size, cluster_coordinates, normalisation):\n",
    "\n",
    "    print(f'>>>>>>>>>>>>> test_size= {test_size}, cluster_coordinates? {cluster_coordinates}, data normalised? {normalisation}')\n",
    "    # preprocessing on input df\n",
    "    df, lnd_ftrs, hbt_ftrs= preprocess_input_data(input_df)\n",
    "\n",
    "    # identify counte regions\n",
    "    counter_regions= find_counter_regions(regions, df)\n",
    "\n",
    "    # train and test split of data\n",
    "    df,train_data,test_data = training_test_split(df, cluster_coordinates, 3, test_size)\n",
    "\n",
    "    # visualise train and test sites on a map\n",
    "    map_sites(df, counter_regions, train_data, test_data)\n",
    "\n",
    "    # calculate low VIF features\n",
    "    low_vif_ftrs= vif(df, lnd_ftrs, hbt_ftrs)\n",
    "\n",
    "    # train model and make predictions\n",
    "    pred_on_train, pred_on_test = predictions(train_data, test_data, low_vif_ftrs, normalisation)\n",
    "\n",
    "    return pred_on_train, pred_on_test\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # run model with parameters that produced best performance in\n",
    "    pred_on_train, pred_on_test= model_development_pipeline(model_input_data, 0.5, True, False)\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "req_old",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "73dd042dec22895802a5cf4c230cd0d0aa33a4e312107f26490806e8c532eb8a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
